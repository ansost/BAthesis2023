{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebe1d924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-27 17:58:00,263.263 DEBUG phonemizer:  Initializing phonemizer with model step 1120000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import buckeye \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dp.phonemizer import Phonemizer\n",
    "\n",
    "phonemizer = Phonemizer.from_checkpoint('/home/anna/anna/data/en_us_cmudict_forward.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26529cde",
   "metadata": {},
   "source": [
    "### Global Speech Rate\n",
    "Defined as number of syllables per utterance divided by the total duration of the utterance.\n",
    "\n",
    "Presupposes a dataframe with pauses, words and two empty columns: 'utteranceID' and 'global_sr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = buckeye.corpus('/mnt/c/Users/astei/ownCloud - Anna Stein@cloud.phil.hhu.de/buckeye_data/zipped_corpus/')\n",
    "df = pd.DataFrame({'items': [], 'trackID': []})\n",
    "num = 0\n",
    "\n",
    "for speaker in tqdm(corpus):\n",
    "    num = num + 1\n",
    "    tracks = {}\n",
    "    trackNum = 0\n",
    "    for track in speaker:\n",
    "        trackWords = []\n",
    "        \n",
    "        for word in track.words: \n",
    "            if isinstance(word, buckeye.containers.Word):\n",
    "                trackWords.append(word)\n",
    "            elif isinstance(word, buckeye.containers.Pause):\n",
    "                pauseType = str(word).split(' ')\n",
    "                trackWords.append(pauseType[1])\n",
    "                \n",
    "        tracks[trackNum] = trackWords\n",
    "        \n",
    "        trackNum = trackNum + 1\n",
    "    \n",
    "    \n",
    "    for entry in tracks.items():\n",
    "        for word in entry[1]:\n",
    "            df.loc[len(df)] = {'items': word, 'trackID': entry[0]}\n",
    "            \n",
    "    print(df)\n",
    "    #df.to_csv('/home/anna/anna/data/temp/s' + str(num) + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a666b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = buckeye.corpus('/mnt/c/Users/astei/ownCloud - Anna Stein@cloud.phil.hhu.de/buckeye_data/zipped_corpus/')\n",
    "for speaker in corpus:\n",
    "    for track in speaker:\n",
    "        for word in track.words:\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0a230b",
   "metadata": {},
   "source": [
    "### working version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27f9fbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [06:15,  9.38s/it]\n"
     ]
    }
   ],
   "source": [
    "corpus = buckeye.corpus('/mnt/c/Users/astei/ownCloud - Anna Stein@cloud.phil.hhu.de/buckeye_data/zipped_corpus/')\n",
    "num = 0\n",
    "pattern = r\"\\{(\\w*)\\}|\\<(\\w*)\\>\"\n",
    "utteranceID = 1\n",
    "inUtterance = bool \n",
    "forbidden_words = ['oh', 'uh', 'ah', 'um', 'mm', 'hm', 'huh', 'uh-huh', 'um-hum', 'huh-uh', 'hum-hum', 'hmm', 'hmmm', 'mh', 'mmh']\n",
    "# Appends all speaker dfs so there are no speaker overlaps while the df is being constructed. \n",
    "all_dfs = []\n",
    "\n",
    "for speaker in tqdm(corpus):\n",
    "    num = num + 1\n",
    "    df_name = str(num) + 'df'\n",
    "    df_name = pd.DataFrame({'items': [], 'utteranceID': [], 'global_sr': []})\n",
    "    speaker_words = []\n",
    "    \n",
    "    for track in speaker:\n",
    "        for word in track.words: \n",
    "            if isinstance(word, buckeye.containers.Word):\n",
    "                speaker_words.append(word)\n",
    "            elif isinstance(word, buckeye.containers.Pause):\n",
    "                pauseType = str(word).split(' ')\n",
    "                speaker_words.append(pauseType[1])\n",
    "    \n",
    "    # Collects words per utterance        \n",
    "    wordsUtterance = []\n",
    "\n",
    "    for index, word in enumerate(speaker_words):\n",
    "\n",
    "        if isinstance(word, buckeye.containers.Word) and word.orthography not in forbidden_words:\n",
    "            inUtterance = True\n",
    "        else:\n",
    "            inUtterance = False\n",
    "\n",
    "        if inUtterance == True: \n",
    "            raw_syllables = stringify(syllabify(English, get_segments(word.orthography, upper = True)))\n",
    "            wordsUtterance.append((index, word.dur, len(raw_syllables.split(' ')), word.orthography))   \n",
    "\n",
    "        elif inUtterance == False and len(wordsUtterance) > 0:\n",
    "            totalDur = sum([item[1] for item in wordsUtterance])\n",
    "            totalSyl = sum([item[2] for item in wordsUtterance])\n",
    "            globalSr = totalSyl / totalDur\n",
    "\n",
    "            for entry in wordsUtterance:\n",
    "                df_name.at[entry[0], 'utteranceID'] = utteranceID\n",
    "                df_name.at[entry[0], 'global_sr'] = globalSr\n",
    "                df_name.at[entry[0], 'items'] = entry[3]\n",
    "\n",
    "            utteranceID = utteranceID + 1\n",
    "            wordsUtterance = []\n",
    "\n",
    "    ## Sanity Test\n",
    "    sprecher = pd.read_csv('/mnt/c/Users/astei/ownCloud - Anna Stein@cloud.phil.hhu.de/buckeye_data/allwords_perspeaker_csv/s' + str(num) + '.csv')\n",
    "    items = df_name['items'].tolist()\n",
    "    tokens = sprecher['token'].tolist()\n",
    "    assert len(tokens) == len(items)\n",
    "    del sprecher, items, tokens\n",
    "\n",
    "    df_name.reset_index(drop=True, inplace=True)\n",
    "    all_dfs.append(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2433de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_dfs) == 40\n",
    "\n",
    "alle = pd.concat(all_dfs)\n",
    "alle.to_csv('/home/anna/anna/data/globalSr_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42bfa829",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = pd.read_csv('/mnt/c/Users/astei/ownCloud - Anna Stein@cloud.phil.hhu.de/R/BA/data/regression_data_otherPrior.csv', \n",
    "                        low_memory = True, \n",
    "                        engine = 'c', \n",
    "                        dtype = {'speakerID': 'category', 'speakerAge': 'category', 'speakerGender': 'category', \n",
    "                                 'interviewerGender': 'category', 'wordPOS': 'category', 'n_segments': 'category', \n",
    "                                 'n_syllables': 'category'})\n",
    "alle_dfs = pd.concat(all_dfs, axis=0)\n",
    "alle_dfs.reset_index(drop=True, inplace=True)\n",
    "finalDF = pd.concat([regression, alle_dfs], axis = 1)\n",
    "finalDF.to_csv('/mnt/c/Users/astei/ownCloud - Anna Stein@cloud.phil.hhu.de/R/BA/data/regression_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f252713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalfinalDF = finalDF.drop('items', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1121442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalfinalDF.to_csv('/mnt/c/Users/astei/ownCloud - Anna Stein@cloud.phil.hhu.de/R/BA/data/regression_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b63ce",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7a32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for getting/formatting segments/syllables\n",
    "def get_segments(word, upper = False):\n",
    "    \"\"\"Returns the segments of a word.\"\"\"\n",
    "    raw_segment_string = phonemizer(str(word), lang='en_us')\n",
    "    \n",
    "    if upper:\n",
    "        segments_string = re.sub(r'[\\[\\]-]',' ', raw_segment_string)\n",
    "        segments = segments_string.split()\n",
    "        return segments\n",
    "    else:\n",
    "        segments_string = re.sub(r'[\\[\\]-]',' ', raw_segment_string.lower())\n",
    "        segments = segments_string.split()\n",
    "        return segments\n",
    "\n",
    "def join_segments(word):\n",
    "    \"\"\"Returns the segments of a word in a cue formatted string.\"\"\"\n",
    "    segments = get_segments(word)\n",
    "    segments_y = []\n",
    "    for segment in segments:\n",
    "        segment = 's.' + segment\n",
    "        segments_y.append(segment)\n",
    "    segments_joined = '_'.join(segments_y)\n",
    "    return segments_joined\n",
    "\n",
    "def join_syllables(syllables):\n",
    "    \"\"\"Returns the syllables of a word in a cue formatted string.\"\"\"\n",
    "    syll_list = syllables.split()\n",
    "    syllable_cuestring = []\n",
    "    for entry in syll_list:\n",
    "        syllable_cue = 'y.' + entry\n",
    "        syllable_cuestring.append(syllable_cue.lower())\n",
    "    syllables_joined = '_'.join(syllable_cuestring)\n",
    "    return syllables_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbd262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Syllabifier script. \n",
    "# English language settings for the language parameter in the syllabifier.\n",
    "English = {\n",
    "    'consonants': ['B', 'CH', 'D', 'DH', 'F', 'G', 'HH', 'JH', 'K', 'L',\n",
    "                   'M', 'N', 'NG', 'P', 'R', 'S', 'SH', 'T', 'TH', 'V', 'W',\n",
    "                   'Y', 'Z', 'ZH'],\n",
    "    'vowels': [ 'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'ER', 'EY', 'IH',\n",
    "               'IY', 'OW', 'OY', 'UH', 'UW'],\n",
    "    'onsets': ['P', 'T', 'K', 'B', 'D', 'G', 'F', 'V', 'TH', 'DH', 'S', 'Z',\n",
    "               'SH', 'CH', 'JH', 'M', 'N', 'R', 'L', 'HH', 'W', 'Y', 'P R',\n",
    "               'T R', 'K R', 'B R', 'D R', 'G R', 'F R', 'TH R', 'SH R',\n",
    "               'P L', 'K L', 'B L', 'G L', 'F L', 'S L', 'T W', 'K W',\n",
    "               'D W','S W', 'S P', 'S T', 'S K', 'S F', 'S M', 'S N', 'G W',\n",
    "               'SH W', 'S P R', 'S P L', 'S T R', 'S K R', 'S K W', 'S K L',\n",
    "               'TH W', 'ZH', 'P Y', 'K Y', 'B Y', 'F Y', 'HH Y', 'V Y',\n",
    "               'TH Y', 'M Y', 'S P Y', 'S K Y', 'G Y', 'HH W', '']\n",
    "    }\n",
    "     \n",
    "def syllabify(language, word):\n",
    "    '''Syllabifies the word, given a language configuration loaded with\n",
    "    loadLanguage. word is either a string of phonemes from the CMU\n",
    "    pronouncing dictionary set (with optional stress numbers after vowels),\n",
    "    or a Python list of phonemes, e.g. \"B AE1 T\" or [\"B\", \"AE1\", \"T\"]\n",
    "    '''\n",
    "\n",
    "    if type(word) == str:\n",
    "        word = word.split()\n",
    "    # This is the returned data structure.\n",
    "    syllables = []\n",
    "\n",
    "    # This maintains a list of phonemes between nuclei.\n",
    "    internuclei = []\n",
    "\n",
    "    for phoneme in word :\n",
    "\n",
    "        phoneme = phoneme.strip()\n",
    "        if phoneme == \"\" :\n",
    "            continue\n",
    "        stress = None\n",
    "        if phoneme[-1].isdigit() :\n",
    "            stress = int(phoneme[-1])\n",
    "            phoneme = phoneme[0:-1]\n",
    "\n",
    "        # Split the consonants seen since the last nucleus into coda and\n",
    "        # onset.\n",
    "        if phoneme in language[\"vowels\"] :\n",
    "\n",
    "            coda = None\n",
    "            onset = None\n",
    "\n",
    "            # If there is a period in the input, split there.\n",
    "            if \".\" in internuclei :\n",
    "                period = internuclei.index(\".\")\n",
    "                coda = internuclei[:period]\n",
    "                onset = internuclei[period+1:]\n",
    "\n",
    "            else :\n",
    "                # Make the largest onset we can. The 'split' variable marks\n",
    "                # the break point.\n",
    "                for split in range(0, len(internuclei)+1) :\n",
    "                    coda = internuclei[:split]\n",
    "                    onset = internuclei[split:]\n",
    "\n",
    "                    # If we are looking at a valid onset, or if we're at the\n",
    "                    # start of the word (in which case an invalid onset is\n",
    "                    # better than a coda that doesn't follow a nucleus), or\n",
    "                    # if we've gone through all of the onsets and we didn't\n",
    "                    # find any that are valid, then split the nonvowels\n",
    "                    # we've seen at this location.\n",
    "                    if \" \".join(onset) in language[\"onsets\"] \\\n",
    "                       or len(syllables) == 0 \\\n",
    "                       or len(onset) == 0 :\n",
    "                       break\n",
    "\n",
    "\n",
    "            # Tack the coda onto the coda of the last syllable. Can't do it\n",
    "            # if this is the first syllable.\n",
    "            if len(syllables) > 0 :\n",
    "                syllables[-1][3].extend(coda)\n",
    "\n",
    "            # Make a new syllable out of the onset and nucleus.\n",
    "            syllables.append( (stress, onset, [phoneme], []) )\n",
    "\n",
    "            # At this point we've processed the internuclei list.\n",
    "            internuclei = []\n",
    "\n",
    "        elif not phoneme in language[\"consonants\"] and phoneme != \".\" :\n",
    "            raise ValueError(\"Invalid phoneme: \" + phoneme)\n",
    "\n",
    "        else : # a consonant\n",
    "            internuclei.append(phoneme)\n",
    "\n",
    "    # Done looping through phonemes. We may have consonants left at the end.\n",
    "    # We may have even not found a nucleus.\n",
    "    if len(internuclei) > 0 :\n",
    "        if len(syllables) == 0 :\n",
    "            syllables.append( (None, internuclei, [], []) )\n",
    "        else :\n",
    "            syllables[-1][3].extend(internuclei)\n",
    "\n",
    "    return syllables\n",
    "\n",
    "def stringify(syllables) :\n",
    "    '''This function takes a syllabification returned by syllabify and\n",
    "       turns it into a string, with phonemes spearated by spaces and\n",
    "       syllables spearated by periods.'''\n",
    "    ret = []\n",
    "    for syl in syllables :\n",
    "        stress, onset, nucleus, coda = syl\n",
    "        if stress != None and len(nucleus) != 0 :\n",
    "            nucleus[0] += str(stress)\n",
    "        ret.append(\"\".join(onset + nucleus + coda))\n",
    "    return \" \".join(ret)\n",
    "\n",
    "language = English"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

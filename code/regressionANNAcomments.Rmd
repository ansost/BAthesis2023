---
title: "regression"
author: "Anna Stein"
date: "2022-12-12"
output: html_document
---


```{R}
for (package in c("here","tidyverse","ggplot2","lme4",'lmerTest', 'car', 'fmsb', 'languageR', 'corrplot','Hmisc', 'ggstatsplot')) {
    if (!require(package, character.only=T, quietly=T)) {
        install.packages(package)
        library(package, character.only=T)
    }
}
here::here()
```

```{r setup, include=FALSE}
# knitr::opts_knit$set(root.dir = "../data/")
```

## Read in dataframe
```{r}
df_initial <- read_csv('../data/regression_data.csv')
summary(df_initial)
str(df_initial)
```

## Removing outliers.
Probably more outliers have to be removed. Thats for later tho:=
```{R}
df.fil = subset(df_initial,wordDur > 0 & wordDur < 10)
```

## Removing function words
Probably more outliers have to be removed. Thats for later tho:=
```{R}
df.fil = subset(df.fil,wordPOS %in% c("JJ","NN","RB","V"))
df = df.fil
```

### Transforming word duration and predictor variables

We should only transform AFTER all filters, as z-scoring depends on the means
```{r}

df$wordDurMs.log10 <- log10(df$wordDur*1000)

z.fun <- function(df,variables) { 
  for (variable in variables) {
  df[paste0(variable,'.z')] <- NULL
  df[paste0(variable,'.z')] <- as.numeric(scale(df[variable]))
  }
  return (df)
}

varbs.for.z = c(
  'n_segments',
  "n_syllables",
  "activation_context",
  "activation_syllables",
  "activation_segments",
  "activation_all",
  "prior_context",
  "prior_syllables",
  "prior_segments",
  "prior_all",
  "global_sr" # added global_sr here
  )
df = z.fun(df,varbs.for.z)


```

## Contrast coding
```{r}
df$speakerGender.f.sc <- invisible(as_factor((df$speakerGender)))
contrasts(df$speakerGender.f.sc) <- c(-0.5,0.5)
colnames(contrasts(df$speakerGender.f.sc)) <- 'm.v.f'
contrasts(df$speakerGender.f.sc)

df$interviewerGender.f.sc <- invisible(as_factor((df$interviewerGender)))
contrasts(df$interviewerGender.f.sc) <- c(-0.5,0.5)
colnames(contrasts(df$interviewerGender.f.sc)) <- 'm.v.f'
contrasts(df$interviewerGender.f.sc)

df$speakerAge.f.sc <- invisible(as_factor((df$speakerAge)))
contrasts(df$speakerAge.f.sc) <- c(-0.5,0.5)
colnames(contrasts(df$speakerAge.f.sc)) <- 'o.v.y'
contrasts(df$speakerAge.f.sc)
```

## POS encoding
```{r}
df$wordPOS.simp = as.character(df$wordPOS)
table(df$wordPOS.simp)
# df$wordPOS.simp = substr(df$wordPOS.simp,1,2)
# table(df$wordPOS.simp)

numPOSSimpType = length(unique(df$wordPOS.simp))
sum_code_POSSimp_mat = contr.sum(numPOSSimpType)/2
df$wordPOS.simp.f.sc = as.factor(as.character(df$wordPOS.simp))

contrasts(df$wordPOS.simp.f.sc) <- sum_code_POSSimp_mat
# 
# constrast_names = paste0(levels(df$wordPOS.simp.f.sc)[1:numPOSSimpType-1],'.vs.',levels(df$wordPOS.simp.factor)[numPOSSimpType])
# colnames(contrasts(df$wordPOS.simp.f.sc)) = constrast_names
# contrasts(df$wordPOS.simp.f.sc)
```

### POS Target Coding
```{r}
library(dplyr)
lookup = df %>%
    group_by(wordPOS) %>%
  summarise(wordPOS.tc = mean(wordDurMs.log10))
df = left_join(df, lookup)
df = z.fun(df,c('wordPOS.tc'))
```
Descriptive
```{r}
desc.stats = function(var){
vec = round(c(mean(var),sd(var),IQR(var),max(var)-min(var)),4)
return (vec)
}
desc.stats(df$wordDurMs.log10)
desc.stats(df$n_segments.z)
desc.stats(df$n_syllables.z)
desc.stats(df$wordPOS.tc.z)
desc.stats(df$activation_context.z)
desc.stats(df$activation_syllables.z)
desc.stats(df$activation_segments.z)
desc.stats(df$activation_all.z)
desc.stats(df$prior_context.z)
desc.stats(df$prior_syllables.z)
desc.stats(df$prior_segments.z)
desc.stats(df$prior_all.z)
desc.stats(df$global_sr.z) # added global_sr here

table(df$speakerGender.f.sc)
table(df$interviewerGender.f.sc)
table(df$speakerAge.f.sc)
```


## Model comparison: Domain separated measures vs. All-in-one measures

Compare models with the All prior and All Activation against the more fine-grained ones with Context, Syllables and Segments. Added global_sr here.
--> The fine-grained model is better because the AIC is lower.  

```{r}
mActA_PriorA <- lmer(wordDurMs.log10 ~ (1|speakerID) + (1|wordID) + 
                n_segments.z + n_syllables.z + 
                speakerGender.f.sc + interviewerGender.f.sc + speakerAge.f.sc +
                wordPOS.tc.z + global_sr.z + 
                activation_all.z+ 
                prior_all.z
              , data=df, REML = TRUE, control=lmerControl(optimizer = 'bobyqa', optCtrl=list(maxfun=2e9)))
summary(mActA_PriorA)

mActCoSySg_PriorCoSySg <- lmer(wordDurMs.log10 ~ (1|speakerID) + (1|wordID) + 
                n_segments.z + n_syllables.z + 
                speakerGender.f.sc + interviewerGender.f.sc + speakerAge.f.sc +
                wordPOS.tc.z + global_sr.z +
                activation_context.z + activation_syllables.z + activation_segments.z + 
                prior_context.z + prior_syllables.z + prior_segments.z
              , data=df, REML = TRUE, control=lmerControl(optimizer = 'bobyqa', optCtrl=list(maxfun=2e9)))
summary(mActCoSySg_PriorCoSySg)

AIC(mActA_PriorA,mActCoSySg_PriorCoSySg)
# The more fine-grained one wins!
```
## Nested Model comparison with ANOVAs

Run the models that we want to compare.
```{r}
mActSySg_PriorCoSySg = update(mActCoSySg_PriorCoSySg, . ~ . -activation_context.z)
mActCoSg_PriorCoSySg = update(mActCoSySg_PriorCoSySg, . ~ . -activation_syllables.z)
mActCoSy_PriorCoSySg = update(mActCoSySg_PriorCoSySg, . ~ . -activation_segments.z)

mActCoSySg_PriorSySg = update(mActCoSySg_PriorCoSySg, . ~ . -prior_context.z)
mActCoSySg_PriorCoSg = update(mActCoSySg_PriorCoSySg, . ~ . -prior_syllables.z)
mActCoSySg_PriorCoSy = update(mActCoSySg_PriorCoSySg, . ~ . -prior_segments.z)

#,control=lmerControl(optimizer="Nelder_Mead",optCtrl=list(maxfun=2e9))
```

### Compare the models with AIC and BIC
If we drop one factor, does the model get worse?
If it gets worse, the factor that was dropped significantly improves model fit.
Significant is not used in a P value sort of way.
Whether it improves model fit can be seen by looking at AIC and BIC. Lower is better. 
You may also look at loglikelihood where higher is better. 
For contradicting AIC and BIC, pray that it doesn't happen.

+ means improves model fit
- means does not improve model fit

--> We drop PriorSegment because it is the worst one aka worst fit according to
AIC BIC loglikelihood and P value.
```{r}
# Dropping Activation measures
anova(mActCoSySg_PriorCoSySg,mActSySg_PriorCoSySg) # ActContext +
anova(mActCoSySg_PriorCoSySg,mActCoSg_PriorCoSySg) # ActSyl +
anova(mActCoSySg_PriorCoSySg,mActCoSy_PriorCoSySg) # ActSegment ~ but generally +

#Dropping Prior measures
anova(mActCoSySg_PriorCoSySg,mActCoSySg_PriorSySg) # PriorContext +
anova(mActCoSySg_PriorCoSySg,mActCoSySg_PriorCoSg) # PriorSyllable -
anova(mActCoSySg_PriorCoSySg,mActCoSySg_PriorCoSy) # PriorSegment -
```


# More comparison with the model that drops PriorSegment: mActCoSySg_PriorCoSy
New_model_name = model_that_won_chunkbefore - measure_to_be_dropped
The model that won the chunk before is the one that drops PriorSegment.
--> ActSegment is now significant, altough it was ambigous before.
--> Nothing else needs ot be dropped, all measures increase model fit. 
```{r}
mActSySg_PriorCoSy = update(mActCoSySg_PriorCoSy, . ~ . -activation_context.z)
mActCoSg_PriorCoSy = update(mActCoSySg_PriorCoSy, . ~ . -activation_syllables.z)
mActCoSy_PriorCoSy = update(mActCoSySg_PriorCoSy, . ~ . -activation_segments.z)

mActCoSySg_PriorSy = update(mActCoSySg_PriorCoSy, . ~ . -prior_context.z)
mActCoSySg_PriorCo = update(mActCoSySg_PriorCoSy, . ~ . -prior_syllables.z)

anova(mActCoSySg_PriorCoSy,mActSySg_PriorCoSy) # ActContext +
anova(mActCoSySg_PriorCoSy,mActCoSg_PriorCoSy) # ActSyllable + 
anova(mActCoSySg_PriorCoSy,mActCoSy_PriorCoSy) # ActSegment + 

anova(mActCoSySg_PriorCoSy,mActCoSySg_PriorSy) # PriorContext + 
anova(mActCoSySg_PriorCoSy,mActCoSySg_PriorCo) # PriorSyllable ~ but +
```

## Model Criticism: Summaries of the best models?
```{r}
summary_mActCoSySg_PriorCoSy <- summary(mActCoSySg_PriorCoSy)
print(summary_mActCoSySg_PriorCoSy, correlation = TRUE)
```

```{r}
summary(mActCoSySg_PriorCo)
```

## More Model Criticism but different
Plot the model outliers. They are BAD at the tails of the distribution.
```{r}
# Create trimmed model 
resid_model = resid(mActCoSySg_PriorCoSy) 
qqp(resid_model)
```

Remove the outliers and see how much data in % we are getting rid of.
```{r}
temp_df = df 

# Trimmed the data points: data points that are 2.5 > than the mean are trimmed 
temp_df_trim = temp_df[abs(scale(resid_model)) < 2.5, ]

# Which are the trimmed data points 
outliers = temp_df[abs(scale(resid_model)) > 2.5, ] 

# % Trimmed data
nrow(outliers)/nrow(temp_df)*100
```

```{r}
mActCoSySg_PriorCoSy_trim = update(mActCoSySg_PriorCoSy, data=temp_df_trim) 
resid_model_aftertrim = resid(mActCoSySg_PriorCoSy_trim)
qqp(resid_model_aftertrim) 
summary(mActCoSySg_PriorCoSy_trim)
```
```{r}
ls()
```


# Exploring surpressor effects
--> Collinearity is relatively small, so thats ok. We are at 9.51, 6 or below is no collinearity,
15 and under is a moderate level of colinearity and 30 is just plain bad. 
```{r}
# Check VIF to see if all the predictors are too correlated?
source('vif_func.R')

df$speakerGender.f.sc.num = as.character(df$speakerGender.f.sc)
df$speakerGender.f.sc.num[df$speakerGender.f.sc == 'm'] = 0.5
df$speakerGender.f.sc.num[df$speakerGender.f.sc == 'f'] = -0.5
df$speakerGender.f.sc.num = as.numeric(df$speakerGender.f.sc.num)

df$interviewerGender.f.sc.num = as.character(df$interviewerGender.f.sc)
df$interviewerGender.f.sc.num[df$interviewerGender.f.sc == 'm'] = 0.5
df$interviewerGender.f.sc.num[df$interviewerGender.f.sc == 'f'] = -0.5
df$interviewerGender.f.sc.num = as.numeric(df$interviewerGender.f.sc.num)

df$speakerAge.f.sc.num = as.character(df$speakerAge.f.sc)
df$speakerAge.f.sc.num[df$speakerAge.f.sc == 'o'] = 0.5
df$speakerAge.f.sc.num[df$speakerAge.f.sc == 'y'] = -0.5
df$speakerAge.f.sc.num = as.numeric(df$speakerAge.f.sc.num)

varb_temp = df[c("n_segments.z",
                "n_syllables.z",
                "speakerGender.f.sc.num",
                "interviewerGender.f.sc.num",
                "speakerAge.f.sc.num",
                "wordPOS.tc.z",
                "global_sr.z",
                "activation_context.z",
                "activation_syllables.z",
                "activation_segments.z",
                "prior_context.z",
                "prior_syllables.z",
                "prior_segments.z")]
vif_func(as.data.frame(varb_temp))
#summary(df$speakerGender.f.sc.num)
#summary(df$interviewerGender.f.sc.num)
#summary(df$speakerAge.f.sc.num)
#str(varb_temp)
as.data.frame(varb_temp)
collin.fnc(varb_temp)
```
## Correlation Matrix
```{r}
varb_temp$wordDurMs.log10 <- df$wordDurMs.log10
corrM <- cor(varb_temp)
round(corrM,2)
```

```{r}
corrplot(corrM, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```
```{r}
sessionInfo()
install.packages('corrplot')
library(corrplot)
ggcorrplot(corrM)
corrplot(corrM, method="number")
summary(mActCoSySg_PriorCoSy_trim)
```
Trying out scatterplots to show effect sizes. 
```{r}
install.packages('ggplot2')
library(ggplot2)
p <- ggscatterstats(
  df,
  x = activation_context.z,
  y = wordDurMs.log10,
)
p

ggplot(df,aes(y=wordDurMs.log10,x=activation_syllables))+
  #geom_point(alpha = 0.2)+
  geom_smooth(method="lm")
#
install.packages('sjPlot')
```

```{r}
library(sjPlot)

plot_model(mActCoSySg_PriorCoSy_trim, type = "pred", terms = 'prior_syllables.z')
```


```{r}
# Trying correlation matrices, they all dont look good.
cex.before <- par("cex")
par(cex = 0.7)
corrplot(cor(corrM), p.mat = cor1[[1]], insig = "blank", method = "color",
    addCoef.col="grey", 
    order = "AOE", tl.cex = 1/par("cex"),
    cl.cex = 1/par("cex"), addCoefasPercent = TRUE)
par(cex = cex.before)
```
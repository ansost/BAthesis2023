{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83954b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 20:55:08,541.541 DEBUG phonemizer:  Initializing phonemizer with model step 1120000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import buckeye\n",
    "import re\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from dp.phonemizer import Phonemizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "phonemizer = Phonemizer.from_checkpoint('/home/ansost/slamndl/notebooks/files/en_us_cmudict_forward.pt')\n",
    "corpus = buckeye.corpus('/home/ansost/buckeye_corpus/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26c950",
   "metadata": {},
   "source": [
    "## Variables\n",
    "- speaker ID --> `speaker.name`\n",
    "- speaker age --> `speaker.age`\n",
    "- speaker gender --> `speaker.sex`\n",
    "- interviewer gender --> `speaker.interviewer`\n",
    "---\n",
    "- word token/ ID --> `word.orthography`\n",
    "- word duration --> `word.dur`\n",
    "- word POS --> `word.pos`\n",
    "- n_segments -->  `transcription = phonemizer(token, lang='en_us')`\n",
    "- n_syllables --> `syllables = stringify(syllabify(English, token))`\n",
    "---\n",
    "- local/global sr ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "016684ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English language settings for the language parameter in the syllabifier.\n",
    "English = {\n",
    "    'consonants': ['B', 'CH', 'D', 'DH', 'F', 'G', 'HH', 'JH', 'K', 'L',\n",
    "                   'M', 'N', 'NG', 'P', 'R', 'S', 'SH', 'T', 'TH', 'V', 'W',\n",
    "                   'Y', 'Z', 'ZH'],\n",
    "    'vowels': [ 'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'ER', 'EY', 'IH',\n",
    "               'IY', 'OW', 'OY', 'UH', 'UW'],\n",
    "    'onsets': ['P', 'T', 'K', 'B', 'D', 'G', 'F', 'V', 'TH', 'DH', 'S', 'Z',\n",
    "               'SH', 'CH', 'JH', 'M', 'N', 'R', 'L', 'HH', 'W', 'Y', 'P R',\n",
    "               'T R', 'K R', 'B R', 'D R', 'G R', 'F R', 'TH R', 'SH R',\n",
    "               'P L', 'K L', 'B L', 'G L', 'F L', 'S L', 'T W', 'K W',\n",
    "               'D W','S W', 'S P', 'S T', 'S K', 'S F', 'S M', 'S N', 'G W',\n",
    "               'SH W', 'S P R', 'S P L', 'S T R', 'S K R', 'S K W', 'S K L',\n",
    "               'TH W', 'ZH', 'P Y', 'K Y', 'B Y', 'F Y', 'HH Y', 'V Y',\n",
    "               'TH Y', 'M Y', 'S P Y', 'S K Y', 'G Y', 'HH W', '']\n",
    "    }\n",
    "     \n",
    "def syllabify(language, word):\n",
    "    '''Syllabifies the word, given a language configuration loaded with\n",
    "    loadLanguage. word is either a string of phonemes from the CMU\n",
    "    pronouncing dictionary set (with optional stress numbers after vowels),\n",
    "    or a Python list of phonemes, e.g. \"B AE1 T\" or [\"B\", \"AE1\", \"T\"]\n",
    "    '''\n",
    "\n",
    "    if type(word) == str:\n",
    "        word = word.split()\n",
    "    # This is the returned data structure.\n",
    "    syllables = []\n",
    "\n",
    "    # This maintains a list of phonemes between nuclei.\n",
    "    internuclei = []\n",
    "\n",
    "    for phoneme in word :\n",
    "\n",
    "        phoneme = phoneme.strip()\n",
    "        if phoneme == \"\" :\n",
    "            continue\n",
    "        stress = None\n",
    "        if phoneme[-1].isdigit() :\n",
    "            stress = int(phoneme[-1])\n",
    "            phoneme = phoneme[0:-1]\n",
    "\n",
    "        # Split the consonants seen since the last nucleus into coda and\n",
    "        # onset.\n",
    "        if phoneme in language[\"vowels\"] :\n",
    "\n",
    "            coda = None\n",
    "            onset = None\n",
    "\n",
    "            # If there is a period in the input, split there.\n",
    "            if \".\" in internuclei :\n",
    "                period = internuclei.index(\".\")\n",
    "                coda = internuclei[:period]\n",
    "                onset = internuclei[period+1:]\n",
    "\n",
    "            else :\n",
    "                # Make the largest onset we can. The 'split' variable marks\n",
    "                # the break point.\n",
    "                for split in range(0, len(internuclei)+1) :\n",
    "                    coda = internuclei[:split]\n",
    "                    onset = internuclei[split:]\n",
    "\n",
    "                    # If we are looking at a valid onset, or if we're at the\n",
    "                    # start of the word (in which case an invalid onset is\n",
    "                    # better than a coda that doesn't follow a nucleus), or\n",
    "                    # if we've gone through all of the onsets and we didn't\n",
    "                    # find any that are valid, then split the nonvowels\n",
    "                    # we've seen at this location.\n",
    "                    if \" \".join(onset) in language[\"onsets\"] \\\n",
    "                       or len(syllables) == 0 \\\n",
    "                       or len(onset) == 0 :\n",
    "                       break\n",
    "\n",
    "\n",
    "            # Tack the coda onto the coda of the last syllable. Can't do it\n",
    "            # if this is the first syllable.\n",
    "            if len(syllables) > 0 :\n",
    "                syllables[-1][3].extend(coda)\n",
    "\n",
    "            # Make a new syllable out of the onset and nucleus.\n",
    "            syllables.append( (stress, onset, [phoneme], []) )\n",
    "\n",
    "            # At this point we've processed the internuclei list.\n",
    "            internuclei = []\n",
    "\n",
    "        elif not phoneme in language[\"consonants\"] and phoneme != \".\" :\n",
    "            raise ValueError(\"Invalid phoneme: \" + phoneme)\n",
    "\n",
    "        else : # a consonant\n",
    "            internuclei.append(phoneme)\n",
    "\n",
    "    # Done looping through phonemes. We may have consonants left at the end.\n",
    "    # We may have even not found a nucleus.\n",
    "    if len(internuclei) > 0 :\n",
    "        if len(syllables) == 0 :\n",
    "            syllables.append( (None, internuclei, [], []) )\n",
    "        else :\n",
    "            syllables[-1][3].extend(internuclei)\n",
    "\n",
    "    return syllables\n",
    "\n",
    "def stringify(syllables) :\n",
    "    '''This function takes a syllabification returned by syllabify and\n",
    "       turns it into a string, with phonemes spearated by spaces and\n",
    "       syllables spearated by periods.'''\n",
    "    ret = []\n",
    "    for syl in syllables :\n",
    "        stress, onset, nucleus, coda = syl\n",
    "        if stress != None and len(nucleus) != 0 :\n",
    "            nucleus[0] += str(stress)\n",
    "        ret.append(\"\".join(onset + nucleus + coda))\n",
    "    return \" \".join(ret)\n",
    "\n",
    "language = English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "040f116d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     speakers\u001b[38;5;241m.\u001b[39mappend(df_name)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Concat all individual speaker dataframes into one dataframe. \u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m regression_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeakers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m regression_data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/ansost/theImpossibleBachelorThesis/regression_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/reshape/concat.py:346\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[1;32m    144\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    along the other axes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/reshape/concat.py:403\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    400\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "forbidden_words = ['uh', 'ah', 'um', 'mm', 'hm', 'huh', 'uh-huh', 'um-hum', 'huh-uh', \n",
    "                   'hum-hum', 'hmm', 'hmmm', 'mh', 'mmh', 'oh']\n",
    "speakers = []\n",
    "\n",
    "for speaker in tqdm(corpus):\n",
    "    # Create new dataframe for speaker with all regression variables. \n",
    "    df_name = speaker.name + 'df'\n",
    "    df_name = pd.DataFrame({'speakerID': [],'speakerAge': [], 'speakerGender': [], 'interviewerGender': [],'wordID': [],\n",
    "                            'wordDur': [], 'wordPOS': [], 'n_segments': [], 'n_syllables': [], 'speechRate':[]})\n",
    "    for track in speaker: \n",
    "        for word in track.words: \n",
    "            if isinstance(word, buckeye.containers.Word) and word.orthography not in forbidden_words:\n",
    "                \n",
    "                # Get the segment count.\n",
    "                segments = phonemizer(word.orthography, lang='en_us')\n",
    "                segments = re.sub(r'[\\[\\]-]',' ', segments)\n",
    "                n_seg = len(segments.split())                \n",
    "                \n",
    "                # Get the syllable count. \n",
    "                syllables = stringify(syllabify(English, segments))\n",
    "                n_syll = len(syllables.split())\n",
    "                \n",
    "                # Append all information to the dataframe as a new row. \n",
    "                df_name.loc[len(df_name)] = {'speakerID': speaker.name, 'speakerAge': speaker.age, \n",
    "                                             'speakerGender': speaker.sex, 'interviewerGender': speaker.interviewer,\n",
    "                                             'wordID': word.orthography, 'wordDur': word.dur, 'wordPOS': word.pos,\n",
    "                                             'n_segments': n_seg, 'n_syllables': n_syll}\n",
    "    speakers.append(df_name)\n",
    "\n",
    "# Concat all individual speaker dataframes into one dataframe. \n",
    "regression_data = pd.concat(speakers)\n",
    "regression_data.to_csv('/home/ansost/theImpossibleBachelorThesis/regression_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ddebfce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speakerID</th>\n",
       "      <th>speakerAge</th>\n",
       "      <th>speakerGender</th>\n",
       "      <th>interviewerGender</th>\n",
       "      <th>wordID</th>\n",
       "      <th>wordDur</th>\n",
       "      <th>wordPOS</th>\n",
       "      <th>n_segments</th>\n",
       "      <th>n_syllables</th>\n",
       "      <th>speechRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s01</td>\n",
       "      <td>y</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>okay</td>\n",
       "      <td>0.405470</td>\n",
       "      <td>NN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s01</td>\n",
       "      <td>y</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>i'm</td>\n",
       "      <td>0.145779</td>\n",
       "      <td>PRP_VBP</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s01</td>\n",
       "      <td>y</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>lived</td>\n",
       "      <td>0.223077</td>\n",
       "      <td>VBN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s01</td>\n",
       "      <td>y</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>in</td>\n",
       "      <td>0.128085</td>\n",
       "      <td>IN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s01</td>\n",
       "      <td>y</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>columbus</td>\n",
       "      <td>0.485544</td>\n",
       "      <td>NNP</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6811</th>\n",
       "      <td>s40</td>\n",
       "      <td>y</td>\n",
       "      <td>m</td>\n",
       "      <td>f</td>\n",
       "      <td>it</td>\n",
       "      <td>0.076823</td>\n",
       "      <td>PRP</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6812</th>\n",
       "      <td>s40</td>\n",
       "      <td>y</td>\n",
       "      <td>m</td>\n",
       "      <td>f</td>\n",
       "      <td>or</td>\n",
       "      <td>0.096032</td>\n",
       "      <td>CC</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6813</th>\n",
       "      <td>s40</td>\n",
       "      <td>y</td>\n",
       "      <td>m</td>\n",
       "      <td>f</td>\n",
       "      <td>not</td>\n",
       "      <td>0.274370</td>\n",
       "      <td>RB</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6814</th>\n",
       "      <td>s40</td>\n",
       "      <td>y</td>\n",
       "      <td>m</td>\n",
       "      <td>f</td>\n",
       "      <td>all</td>\n",
       "      <td>0.294289</td>\n",
       "      <td>DT</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>s40</td>\n",
       "      <td>y</td>\n",
       "      <td>m</td>\n",
       "      <td>f</td>\n",
       "      <td>right</td>\n",
       "      <td>0.253473</td>\n",
       "      <td>NN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276845 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     speakerID speakerAge speakerGender interviewerGender    wordID   wordDur  \\\n",
       "0          s01          y             f                 f      okay  0.405470   \n",
       "1          s01          y             f                 f       i'm  0.145779   \n",
       "2          s01          y             f                 f     lived  0.223077   \n",
       "3          s01          y             f                 f        in  0.128085   \n",
       "4          s01          y             f                 f  columbus  0.485544   \n",
       "...        ...        ...           ...               ...       ...       ...   \n",
       "6811       s40          y             m                 f        it  0.076823   \n",
       "6812       s40          y             m                 f        or  0.096032   \n",
       "6813       s40          y             m                 f       not  0.274370   \n",
       "6814       s40          y             m                 f       all  0.294289   \n",
       "6815       s40          y             m                 f     right  0.253473   \n",
       "\n",
       "      wordPOS  n_segments  n_syllables  speechRate  \n",
       "0          NN           3            2         NaN  \n",
       "1     PRP_VBP           2            1         NaN  \n",
       "2         VBN           4            1         NaN  \n",
       "3          IN           2            1         NaN  \n",
       "4         NNP           8            3         NaN  \n",
       "...       ...         ...          ...         ...  \n",
       "6811      PRP           2            1         NaN  \n",
       "6812       CC           2            1         NaN  \n",
       "6813       RB           3            1         NaN  \n",
       "6814       DT           2            1         NaN  \n",
       "6815       NN           3            1         NaN  \n",
       "\n",
       "[276845 rows x 10 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
